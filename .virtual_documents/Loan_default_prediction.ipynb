











# Importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic("matplotlib", " inline")





# Loading dataset and viewing first few rows
df = pd.read_csv("Default_Fin.csv")
df.head()


# Checking dataset shape and types
df.info()


# Summary statistics
df.describe()


# Checking for missing values
df.isnull().sum()


# Checking for duplicates
df.duplicated().sum()





# Dropping Index column
print(df.columns)


# Renaming column for easier access
df.rename(columns={"Defaulted?": "Defaulted"}, inplace=True)

# Converting binary flags to categorical
df['Employed'] = df['Employed'].astype('category')
df['Defaulted'] = df['Defaulted'].astype('category')

# Confirming changes
df.info()








# Plotting class distribution
sns.countplot(x='Defaulted', data=df)
plt.title("Distribution of Loan Default (Target Variable)")
plt.xlabel("Defaulted")
plt.ylabel("Count")
plt.show()

# Percentage breakdown
default_rate = df['Defaulted'].value_counts(normalize=True) * 100
print("Default Rate Breakdown (%):\n", default_rate.round(2))








# Creating plots
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Histogram and KDE for Bank Balance 
sns.histplot(df['Bank Balance'], kde=True, bins=30, ax=axes[0])
axes[0].set_title("Distribution of Bank Balance")
axes[0].set_xlabel("Bank Balance")
axes[0].set_ylabel("Frequency")

# Histogram and KDE for Annual Salary 
sns.histplot(df['Annual Salary'], kde=True, bins=30, ax=axes[1])
axes[1].set_title("Distribution of Annual Salary")
axes[1].set_xlabel("Annual Salary")
axes[1].set_ylabel("Frequency")

# Display
plt.tight_layout()
plt.show()








# Creating plots
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Boxplot for Bank Balance vs Defaulted
sns.boxplot(x='Defaulted', y='Bank Balance', data=df, ax=axes[0])
axes[0].set_title("Bank Balance by Default Status")

# Boxplot for Annual Salary vs Defaulted 
sns.boxplot(x='Defaulted', y='Annual Salary', data=df, ax=axes[1])
axes[1].set_title("Annual Salary by Default Status")

# Display
plt.tight_layout()
plt.show()








sns.countplot(x='Employed', hue='Defaulted', data=df)
plt.title("Default Rate by Employment Status")
plt.xlabel("Employed")
plt.ylabel("Count")
plt.legend(title='Defaulted')
plt.show()





# Converting 'Employed' and 'Defaulted' from category to numeric codes
df['Employed'] = df['Employed'].cat.codes
df['Defaulted'] = df['Defaulted'].cat.codes





# Standardization

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Applying standard scaling to numerical features
df[['Bank Balance', 'Annual Salary']] = scaler.fit_transform(df[['Bank Balance', 'Annual Salary']])





from sklearn.model_selection import train_test_split

# Features and target
X = df.drop(columns='Defaulted')
y = df['Defaulted']

# Splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)





from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# # Instantiate model
model = LogisticRegression()

# Fitting on training data
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Evaluation
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))





# Confusion matrix values
conf_matrix = confusion_matrix(y_test, y_pred)

# Visualization
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Default', 'Defaulted'], yticklabels=['No Default', 'Defaulted'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()








from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# SMOTE Oversampling
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check new class distribution
print(y_train_smote.value_counts())


# Retraining the model on the oversampled data
model_smote = LogisticRegression()
model_smote.fit(X_train_smote, y_train_smote)


# Predictions
y_pred_smote = model_smote.predict(X_test)

# Evaluation
print("Confusion Matrix (SMOTE):")
print(confusion_matrix(y_test, y_pred_smote))
print("\nClassification Report (SMOTE):")
print(classification_report(y_test, y_pred_smote))





# Undersampling the majority class
undersample = RandomUnderSampler(random_state=42)
X_train_under, y_train_under = undersample.fit_resample(X_train, y_train)

# Checking new class distribution
print(y_train_under.value_counts())


# Retraining the model on the undersampled data
model_under = LogisticRegression()
model_under.fit(X_train_under, y_train_under)

# Predictions
y_pred_under = model_under.predict(X_test)

# Evaluation
print("Confusion Matrix (Undersampling):")
print(confusion_matrix(y_test, y_pred_under))
print("\nClassification Report (Undersampling):")
print(classification_report(y_test, y_pred_under))





from sklearn.metrics import ConfusionMatrixDisplay

# Confusion matrices
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Original Model 
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred), display_labels=[0,1]).plot(ax=axes[0])
axes[0].set_title("Original Model")

# SMOTE Oversampling Model
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_smote), display_labels=[0,1]).plot(ax=axes[1])
axes[1].set_title("SMOTE Oversampling")

# Undersampling Model
ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_under), display_labels=[0,1]).plot(ax=axes[2])
axes[2].set_title("Random Undersampling")

plt.tight_layout()
plt.show()








# Training the model
from sklearn.ensemble import RandomForestClassifier

# Instantiate and training the model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_smote, y_train_smote)


# Predict on test set
y_pred_rf = rf_model.predict(X_test)


# Confusion matrix
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["No Default", "Default"], yticklabels=["No Default", "Default"])
plt.title("Random Forest - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Classification report
print("Classification Report (Random Forest with SMOTE):")
print(classification_report(y_test, y_pred_rf))

# Accuracy
print("Accuracy Score:", accuracy_score(y_test, y_pred_rf))








from sklearn.metrics import roc_curve, roc_auc_score

# Getting predicted probabilities using our best model(Random Forest with SMOTE)
y_probs = rf_model.predict_proba(X_test)[:, 1]

# Calculating the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)

# Calculating the AUC Score
auc_score = roc_auc_score(y_test, y_probs)

print(f"AUC Score: {auc_score:.2f}")





plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {auc_score:.2f})", color='blue')
plt.plot([0, 1], [0, 1], 'k--', label="Random Model")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate (Recall)")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend()
plt.grid(True)
plt.show()








# Threshold tuning

from sklearn.metrics import precision_recall_curve, f1_score

# Getting predicted probabilities for class 1 (defaulter)
y_scores = model.predict_proba(X_test)[:, 1]

# Calculating precision, recall, thresholds
precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)

# Calculating F1 for each threshold
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)

# Plotting Precision-Recall vs Threshold
plt.figure(figsize=(10, 5))
plt.plot(thresholds, precisions[:-1], label="Precision")
plt.plot(thresholds, recalls[:-1], label="Recall")
plt.plot(thresholds, f1_scores[:-1], label="F1 Score", linestyle='--')
plt.xlabel("Decision Threshold")
plt.ylabel("Score")
plt.title("Precision, Recall, and F1 Score vs Threshold")
plt.legend()
plt.grid(True)

# Display
plt.show()








# Getting probabilities for the positive class (1 = Defaulter)
y_scores = model.predict_proba(X_test)[:, 1]

# Getting precision, recall, thresholds
precisions, recalls, thresholds = precision_recall_curve(y_test, y_scores)

# Calxulating F1 scores for each threshold
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)

# Finding index of the best threshold (highest F1)
best_index = np.argmax(f1_scores)
best_threshold = thresholds[best_index]
print(f"Optimal Threshold (F1-Score): {best_threshold:.2f}")


# Applying the new threshold to convert probabilities into class predictions
y_pred_optimal = (y_scores >= best_threshold).astype(int)

# Evaluating the model using the new predictions
print("Confusion Matrix (Optimal Threshold):")
print(confusion_matrix(y_test, y_pred_optimal))

print("\nClassification Report (Optimal Threshold):")
print(classification_report(y_test, y_pred_optimal))








import pandas as pd

# Model performance summary
model_results = pd.DataFrame({
    'Model': ['Baseline Logistic Regression', 'SMOTE + Logistic Regression', 'SMOTE + Random Forest'],
    'Accuracy': [0.973, 0.86, 0.905],
    'Precision (1)': [0.72, 0.17, 0.22],
    'Recall (1)': [0.31, 0.88, 0.73],
    'F1-Score (1)': [0.44, 0.29, 0.34],
    'AUC Score': [None, None, 0.91]  # Only computed for Random Forest with SMOTE
})

model_results.set_index("Model", inplace=True)
model_results











# Plotting feature importance from the Random Forest model
importances = rf_model.feature_importances_
features = X_train.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': importances}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(data=importance_df, x='Importance', y='Feature')
plt.title("Feature Importance - Random Forest")
plt.tight_layout()
plt.show()












